---
title: Stream data ingestion with Redpanda
date: YYYY-MM-DD HH:MM:SS +09:00
categories: [ë°±ì—”ë“œ, Kafka]
tags:
  [
    ë°±ì—”ë“œ,
    Kafka,
    RedPanda,
    MySql,
    Docker,
    MinIO
  ]

toc: true
toc_sticky: true

---
# Stream data ingestion with Redpanda

> ì¶œì²˜  
> [DataOps 01: Stream data ingestion with Redpanda](https://medium.com/@ongxuanhong/dataops-01-stream-data-ingestion-with-redpanda-56b7fd768887)

# Redpanda
## ì¥ì 
- ê´€ë¦¬ê°€ ì‰¬ìš¸ ë¿ë§Œ ì•„ë‹ˆë¼ í”Œë«í¼ êµ¬ì¶• ë¹„ìš©ë„ ì €ë ´í•˜ê¸° ë•Œë¬¸ì— ì‹œìŠ¤í…œ ê´€ë¦¬ìì˜ ì‘ì—…ë„ ëœ ì–´ë µë‹¤.
- Redpandaë¥¼ 10ë°° ë” ë¹ ë¥´ê³  6ë°° ë” ì €ë ´í•˜ê²Œ ë§Œë“œëŠ” ë‘ ê°€ì§€ ì£¼ìš” ê¸°ëŠ¥ì€
**Zookeeperë¥¼ ì œê±°**í•˜ê³  **C++ë¡œ ì½”ë”©ëœ Raft** ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ JVMì„ ê±´ë“œë¦´ í•„
ìš”ê°€ ì—†ë‹¤.
- RedpandaëŠ” 200ì‹œê°„ì˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ í†µí•´ ì‹¤í—˜ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë˜ì—ˆë‹¤.
- RedpandaëŠ” Kafka connectì™€ ê°™ì€ Kafka ìƒíƒœê³„ì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ëœë‹¤.

#### ë°ì´í„° ìˆ˜ì§‘ì— ì‚¬ìš©í•  ë¸Œë¡œì»¤ë¡œ Redpandaë¥¼ ì„¤ì¹˜í•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. 
#### ì—¬ê¸° GitHub ë§í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬í˜„í•  ìˆ˜ ìˆë‹¤:
- [https://github.com/ongxuanhong/de01-stream-ingestion-redpanda-minio](https://github.com/ongxuanhong/de01-stream-ingestion-redpanda-minio)
- ë°ì´í„° ì†ŒìŠ¤ëŠ”
    - ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ìš´ì˜ ë°ì´í„°, íŠ¹íˆ ì‚¬ìš©ìì˜ ì£¼ë¬¸ íŠ¸ëœì­ì…˜ì„ ì—ë®¬ë ˆì´íŠ¸í•˜ëŠ” MySQLì´
    ë‹¤.
    - ë˜í•œ ì´ì»¤ë¨¸ìŠ¤ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‚¬ìš©ì ìƒí˜¸ ì‘ìš© í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë¦­ìŠ¤
    íŠ¸ë¦¼ ì´ë²¤íŠ¸ê°€ ìˆì„ ê²ƒì´ë‹¤.
- ëŒ€ìƒ ì‹±í¬ ë¶„ì‚° ìŠ¤í† ë¦¬ì§€ë¡œ S3, GCS ë˜ëŠ” Azure Blobì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë…ìê°€ í´ë¼ìš°ë“œ ì œê³µì—…ì²´ì— ê³„ì •ì„ ë§Œë“¤ì§€ ì•Šê³ ë„ ì‰½ê²Œ ì„¤ì¹˜í• 
    ìˆ˜ ìˆê¸°ë¥¼ ì›í–ˆê¸° ë•Œë¬¸ì— **ëŒ€ì‹  MinIO**ë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤.
- ì†ŒìŠ¤/ì‹±í¬ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ê¸° ìœ„í•´ MySQLìš© debeziumê³¼ MinIOìš© Kafka connect
ë¥¼ ì„¤ì¹˜í–ˆë‹¤.

# Data sources

## MySQL - operational data

- We use docker-compose.yml to bulid MySQL service

```yaml
mysql:
    image: debezium/example-mysql:1.6
    container_name: mysql
    volumes:
      - ./mysql/data:/var/lib/mysql
    ports:
      - "3306:3306"
    env_file:
      - .env
```

- `.env` íŒŒì¼

```yaml
MYSQL_ROOT_PASSWORD="debezium"
MYSQL_USER="admin"
MYSQL_PASSWORD="admin123"
```

ì—¬ê¸°ì„œëŠ” debeziumì´ ë°ì´í„°ì— ë™ê¸°ì ìœ¼ë¡œ ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆë„ë¡ ì´ë¯¸ CDC ì„¤ì •ì´ ìˆëŠ” debezium/example-mysql:1.6ì´ë¼ëŠ” ì´ë¦„ì˜ ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•œë‹¤.

ë§ì€ ëª…ë ¹ì„ ì…ë ¥í•˜ëŠ” ë° ì‹œê°„ì„ ë‚­ë¹„í•˜ê³  ì‹¶ì§€ ì•Šê¸° ë•Œë¬¸ì— ì‹œìŠ¤í…œê³¼ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆëŠ” ëª…ë ¹ì˜ ë°”ë¡œ ê°€ê¸°ê°€ í¬í•¨ëœ Makefile íŒŒì¼ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

## Makefile

```yaml
include .env

build:
  docker-compose build

up:
  docker-compose --env-file .env up -d

down:
  docker-compose --env-file .env down

ps:
  docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

to_redpanda:
  open http://localhost:8080/topics

to_minio:
  open http://localhost:9001/buckets

to_mysql:
  docker exec -it mysql mysql -u"root" -p"${MYSQL_ROOT_PASSWORD}" ${MYSQL_DATABASE}

to_data_generator:
  docker exec -it data_generator /bin/bash
```

## Spawn data infrastructure

```
make up

# checking redpanda console
make to_redpanda

# checking minio
make to_minio
```

![Untitled](/assets/img/2023-10-40/Untitled.png)

## **Prepare MySQL data source**

ì£¼ë¬¸ ê±°ë˜ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ olist_orders_dataset í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ë¸Œë¼ì§ˆ_ì „ììƒê±°ë˜ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•œë‹¤.

- ëª¨ë“  ë°ì´í„°ëŠ” Kaggleë¡œ ì´ë™í•˜ì—¬ ë“±ë¡í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆë‹¤.
- í¸ì˜ë¥¼ ìœ„í•´ Github ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì—ˆë‹¤.
- ë‹¤ìŒìœ¼ë¡œ `make up` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ MySQL ì„œë¹„ìŠ¤ë¥¼ ë¹Œë“œí•˜ê³  `make` `to_mysql` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ MySQLì— ì ‘ì†í•œ í›„ ì•„ë˜ì™€ ê°™ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰
í•œë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%201.png)

![Untitled](/assets/img/2023-10-40/Untitled%202.png)

# ë°ì´ì„œ ìƒì„± Scripts

* ë‹¤ìŒìœ¼ë¡œ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ í¬í•¨ëœ src/ë¥¼
* ë‹¤ìŒê³¼ ê°™ì€ íŒŒì¼/í´ë” êµ¬ì¡°ë¡œ ìƒì„±í•œë‹¤.

```
src
â”œâ”€â”€ 01_generate_orders.py
â”œâ”€â”€ 02_generate_clickstream.py
â”œâ”€â”€ data
â”‚   â””â”€â”€ olist_orders_dataset.csv
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup_connectors.sh
```

- **01_generate_orders.py** : used to generate transaction data for MySQL.
- **02_generate_clickstream.py** : used to generate clickstream events data.
- **data/olist_orders_dataset.csv**: contains transaction data.
- **Dockerfile** : used to package all the src/ code and necessary requirements.
- **requirements.txt** : contains a list of installed libraries.
- **setup_connectors.sh** : contains requests used to create source/sink connections
for Kafka connect.

## src/ìš© ë„ì»¤ ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ë ¤ë©´ docker-compose.ymlì— ë‹¤ìŒ ì„ ì–¸ì„ ì¶”ê°€í•œë‹¤.

```docker
data-generator:
    build:
      context: ./src
      dockerfile: ./Dockerfile
    container_name: data_generator
    volumes:
      - ./src:/opt/src
    env_file:
      - .env
```

## ê·¸ëŸ° ë‹¤ìŒ makeë¥¼ ì‚¬ìš©í•˜ì—¬ MySQLìš© íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ìƒì„±í•œ
ë‹¤.

```docker
make build
make down
make up
make to_data_generator
```

## to_data_generator ì»¨í…Œì´ë„ˆì— ë“¤ì–´ê°ˆ ë•Œ íŒŒì´ì¬ì„ ì‚¬ìš©í•˜ì—¬
* 01_generate_orders.py ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•œë‹¤. ì„±ê³µí•˜ë©´ ì•„ë˜ì™€ ìœ ì‚¬í•œ ì¶œë ¥ì´ í‘œì‹œëœë‹¤.

## **Generation scripts**

``` 
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python src/01_generate_orders.py
```

ğŸ’¡ ERROR:


![Untitled](/assets/img/2023-10-40/Untitled%203.png)


* íŒ¨í‚¤ì§€ê°€ ì‹œìŠ¤í…œ ì „ì²´ Pythonì— ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ê°€ìƒ í™˜ê²½ì— ë‹¤ì‹œ ì„¤ì¹˜ë˜ì§€ ì•Šê³  ìš”êµ¬ ì‚¬í•­ì´ ì´ë¯¸ ì¶©ì¡±ë˜ì—ˆìŒì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤.

* ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ `pip install` ëª…ë ¹ì„ ì‹¤í–‰í•  ë•Œ `--ignore-installed` í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ í”Œë˜ê·¸ëŠ” ì‹œìŠ¤í…œ ì „ì²´ Pythonì— ì´ë¯¸ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ë„ pipê°€ íŒ¨í‚¤ì§€ë¥¼ ë‹¤ì‹œ ì„¤ì¹˜í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%204.png)


* `01_generated_orders.py` íŒŒì¼ì˜ MySQL HOST ì •ë³´ë¥¼ ìˆ˜ì •í•œë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%205.png)

```docker
(74254, 9)
NO. DATES: 366
Writing data on: 2017-08-01
-Records: 165
Writing data on: 2017-08-02
-Records: 157
Writing data on: 2017-08-03
-Records: 148
```

![Untitled](/assets/img/2023-10-40/Untitled%206.png)

## We return to MySQL to check the generated data

![Untitled](/assets/img/2023-10-40/Untitled%207.png)

# Ingestion layer : Redpanda

## Redpanda â€” fast storage for real-time data streaming

- CDC ë°ì´í„°ë¥¼ í™•ë³´í•œ í›„, ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì½ê¸° ìœ„í•´ Redpandaë¥¼ ê³„ì† ì„¤ì¹˜í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì„ ì–¸ì„ ê³„ì† ì¶”ê°€í•©ë‹ˆë‹¤.

```docker
redpanda:
    image: vectorized/redpanda
    container_name: redpanda
    ports:
      - "9092:9092"
      - "29092:29092"
    command:
      - redpanda
      - start
      - --overprovisioned
      - --smp
      - "1"
      - --memory
      - "1G"
      - --reserve-memory
      - "0M"
      - --node-id
      - "0"
      - --kafka-addr
      - PLAINTEXT://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092
      - --advertise-kafka-addr
      - PLAINTEXT://redpanda:29092,OUTSIDE://redpanda:9092
      - --check=false

  redpanda-console:
    image: vectorized/console
    container_name: redpanda_console
    depends_on:
      - redpanda
    ports:
      - "8080:8080"
    env_file:
      - .env
```

- make down && make upìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ë‹¤ì‹œ ì‹œì‘í•œë‹¤. ì„±ê³µí•˜ë©´ [http://localhost:8080/topics](http://localhost:8080/topics) ì— ì ‘ì†í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ redpanda console ì¸í„°í˜ì´ìŠ¤ê°€ í‘œì‹œë©ë‹ˆë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%208.png)

# Ingestion layer : Kafka connect

## Kafka connect â€” transfer data from source to sink

- ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ Redpandaì— í† í”½ì„ ìƒì„±í•˜ê³ , ì´ í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ì½ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‘ì„±í•˜ê³ , MinIOì— ì €ì¥í•˜ëŠ” ë“±ì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì²˜ìŒë¶€í„° ì™„ì „íˆ ìì²´ ì½”ë”©í•  ìˆ˜ ìˆë‹¤.
- ì†ŒìŠ¤ì—ì„œ ì‹±í¬ê¹Œì§€ ì—°ê²°ì„ ì§€ì›í•˜ëŠ” ìˆ˜ë§ì€ ì»¤ë„¥í„°ì™€ Kafkaë¥¼ ì—°ê²°í•˜ì—¬ ì‘ì—… ì„¤ì •ê³¼ ì‹¤í–‰ì—ë§Œ ì§‘ì¤‘í•¨ìœ¼ë¡œì¨ ë³µì¡í•œ ì„¤ì¹˜ ì‹œê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆë‹¤.
- Kafka ì—°ê²°ì˜ ê¸°ë³¸ Docker ì´ë¯¸ì§€ëŠ” `io.confluent.connect.s3.S3SinkConnector`ê°€ ì—†ìœ¼ë¯€ë¡œ, Kafka ì—°ê²°ìš© ì»¤ë„¥í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ Dockerfileë¡œ kafka/ í´ë”ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

```docker
FROM debezium/connect

RUN curl -O https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connects3/versions/10.3.1/confluentinc-kafka-connect-s3-10.3.1.zip \
	&& unzip confluentinc-kafka-connect-s3-10.3.1.zip \
	&& mv confluentinc-kafka-connect-s3-10.3.1 /kafka/connect/ \
	&& rm confluentinc-kafka-connect-s3-10.3.1.zip
```

We add docker-compose.yml with declarations like below

```docker
kafka-connect:
    build:
      context: ./kafka
      dockerfile: ./Dockerfile
    container_name: kafka_connect
    depends_on:
      - redpanda
    ports:
      - "8083:8083"
    env_file:
      - .env
```

- After make build && make down && make up we test the Kafka connect service by requesting to **localhost:8083/connector-plugins/**

```
{'class': 'io.confluent.connect.s3.S3SinkConnector', 'type': 'sink', 'version': '10.3.1'},
{'class': 'io.confluent.connect.storage.tools.SchemaSourceConnector', 'type': 'source', 'version': '3.4.0'},
{'class': 'io.debezium.connector.db2.Db2Connector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.mongodb.MongoDbConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.mysql.MySqlConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.oracle.OracleConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.postgresql.PostgresConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.spanner.SpannerConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.sqlserver.SqlServerConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'io.debezium.connector.vitess.VitessConnector', 'type': 'source', 'version': '2.2.0.Alpha3'},
{'class': 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector', 'type': 'source', 'version': '3.4.0'},
{'class': 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', 'type': 'source', 'version': '3.4.0'},
{'class': 'org.apache.kafka.connect.mirror.MirrorSourceConnector', 'type': 'source', 'version': '3.4.0'}
```

# Ingestion layer : Debezium

## Debezium â€” read Change Data Capture from MySQL

- ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ë ¤ë©´ Kafka ì—°ê²°ìš© ì»¤ë„¥í„°ë¥¼ ìƒì„±í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤. ì»¤ë„¥í„°ì˜ ì´ë¦„ì€ `io.debezium.connector.mysql.MySqlConnector`ì…ë‹ˆë‹¤.
- kafka _connect ì»¨í„°ì´ë„ˆì— ì ‘ì†í•˜ì—¬ ì»¤ë„¥í„°ë¥¼ ìƒì„±í•œë‹¤.

```
curl --request POST \
--url http://localhost:8083/connectors \
--header 'Content-Type: application/json' \
--data '{
"name": "src-brazillian-ecommerce",
"config": {
"connector.class": "io.debezium.connector.mysql.MySqlConnector",
"tasks.max": "1",
"database.hostname": "mysql",
"database.port": "3306",
"database.user": "debezium",
"database.password": "dbz",
"database.server.id": "184054",
"database.include.list": "brazillian_ecommerce",
"topic.prefix": "dbserver1",
"schema.history.internal.kafka.bootstrap.servers": "redpanda:9092",
"schema.history.internal.kafka.topic": "schema-changes.brazillian_ecommerce"
}
}'
```

ìš”ì²­ì´ ì„±ê³µí•˜ë©´ Redpanda ì½˜ì†”ë¡œ ëŒì•„ê°€ë©´ ë””ë¹„ì§€ì›€ ê´€ë ¨ í† í”½ ì´ ìë™ìœ¼ë¡œ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%209.png)

`dbserver1.brazilian_ecommerce.olist_orders_dataset` í•­ëª©ì„ í™•ì¸í•˜ë©´ CDC ë©”ì‹œì§€ì˜ ì „ì²´ ë‚´ìš©ì„ ë³¼ ìˆ˜ ìˆë‹¤.

- ìˆ˜ì§‘ëœ ì½˜í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ PostgreSQL, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ Redshift ë˜ëŠ” ë°ì´í„° ë ˆì´í¬í•˜ìš°ìŠ¤ì™€ ê°™ì€ ë‹¤ì–‘í•œ ëŒ€ìƒ ì‹±í¬ì— ë³µì œí•  ìˆ˜ ìˆë‹¤.

![Untitled](/assets/img/2023-10-40/Untitled%2010.png)

![Untitled](/assets/img/2023-10-40/Untitled%2011.png)

# Target sink: MinIO

## MinIO - data lake for distributed data storage

- Finally, the **destination** of the data will be MinIO. We add the declarations for MinIO
as below to docker-compose.yml

```docker
minio:
    hostname: minio
    image: "minio/minio"
    container_name: minio
    ports:
      - "9001:9001"
      - "9000:9000"
    command: [ "server", "/data", "--console-address", ":9001" ]
    volumes:
      - ./minio/data:/data
    env_file:
      - .env

  mc:
    image: minio/mc
    container_name: mc
    hostname: mc
    environment:
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_REGION=us-east-1
    entrypoint: >
      /bin/sh -c " until (/usr/bin/mc config host add minio http://minio:9000 minio minio123) do echo '...waiting...' && sleep 1; done; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; exit 0; "
    depends_on:
      - minio
```

- along with the environment variables added to the `.env` file

```docker
# MinIO
MINIO_ROOT_USER="minio"
MINIO_ROOT_PASSWORD="minio123"
MINIO_ACCESS_KEY="minio"
MINIO_SECRET_KEY="minio123"
```

- After make down && make up. We access **[http://localhost:9001/buckets](http://localhost:9001/buckets)** and we will see the interface presented as shown below

![Untitled](/assets/img/2023-10-40/Untitled%2012.png)

# Target sink : Sink CDC

## Sink CDC data into MinIO

- We use Kafka connect to sync data directly to MinIO. Detailed setup information of
`io.confluent.connect.s3.S3SinkConnector`, you can find here .
- [https://docs.confluent.io/kafka-connectors/s3-sink/current/configuration_options.htm](https://docs.confluent.io/kafka-connectors/s3-sink/current/configuration_options.htm)
- kafka_connect ì»¨í…Œì´ë„ˆì— ì ‘ì†í•˜ì—¬ ì»¤ë„¥í„°ë¥¼ ìƒì„±í•œë‹¤.

```docker
curl --request POST \
--url http://localhost:8083/connectors \
--header 'Content-Type: application/json' \
--data '{
"name": "sink-s3-brazillian-ecommerce", 
"config": {
"topics.regex": "dbserver1.brazillian_ecommerce.*",
"topics.dir": "brazillian_ecommerce",
"connector.class": "io.confluent.connect.s3.S3SinkConnector",
"key.converter": "org.apache.kafka.connect.json.JsonConverter",
"value.converter": "org.apache.kafka.connect.json.JsonConverter",
"format.class": "io.confluent.connect.s3.format.json.JsonFormat",
"flush.size": "1000",
"store.url": "http://minio:9000",
"storage.class": "io.confluent.connect.s3.storage.S3Storage",
"s3.region": "us-east-1",
"s3.bucket.name": "warehouse", 
"aws.access.key.id": "minio",
"aws.secret.access.key": "minio123"
}
}'
```

![Untitled](/assets/img/2023-10-40/Untitled%2013.png)

- Check the information on the Redpanda console, we will see a new consumer group is created

![Untitled](/assets/img/2023-10-40/Untitled%2014.png)

- When the data synchronization is complete, we will see the synced files in MinIO

# Same for clickstream data

## Clickstream â€” user event data

We do the same by accessing the data_generator container via make to_data_generator. Then use python to run the script 02_generate_clickstream.py when successful we will see the log as below

![Untitled](/assets/img/2023-10-40/Untitled%2015.png)

![Untitled](/assets/img/2023-10-40/Untitled%2016.png)

At this point, when checking with the Redpanda console, we will see that a topic has clickstream_events been created, along with events captured on the system.

![Untitled](/assets/img/2023-10-40/Untitled%2017.png)

Finally, to sink data about MinIO, register a connector on Kafka connect by requesting as below

```docker
curl --request POST \
	--url http://localhost:8083/connectors \
	--header 'Content-Type: application/json' \
	--data '{
	"name": "sink-s3-clickstream",
	"config": {
	"topics": "clickstream_events",
	"topics.dir": "clickstream_events",
	"connector.class": "io.confluent.connect.s3.S3SinkConnector",
	"key.converter.schemas.enable": "false",
	"key.converter": "org.apache.kafka.connect.json.JsonConverter",
	"value.converter": "org.apache.kafka.connect.json.JsonConverter",
	"value.converter.schemas.enable": "false",
	"s3.compression.type": "gzip",
	"format.class": "io.confluent.connect.s3.format.json.JsonFormat",
	"flush.size": "100",
	"store.url": "http://minio:9000",
	"storage.class": "io.confluent.connect.s3.storage.S3Storage",
	"s3.region": "us-east-1",
	"s3.bucket.name": "warehouse",
	"aws.access.key.id": "minio",
	"aws.secret.access.key": "minio123"
	}
}'
```

Yes, finally clickstream data has been synced to MinIO

![Untitled](/assets/img/2023-10-40/Untitled%2018.png)

# ê²°ë¡ 

* í•œ ì‹œìŠ¤í…œì—ì„œ ë‹¤ë¥¸ ì‹œìŠ¤í…œìœ¼ë¡œ ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ê²ƒì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³  ì¼ê´„ ì²˜ë¦¬ ìˆ˜ì§‘ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë©´ ì›ë˜ ì‹œìŠ¤í…œì— ì •ì²´ê°€ ë°œìƒí•©ë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•´ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ ìš”ì²­ì´ ì‡„ë„í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ 1-2ê°œì˜ í…Œì´ë¸”ì´ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ë³´ê³ ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ë” ë§ì€ í…Œì´ë¸”ì´ í•„ìš”í•©ë‹ˆë‹¤.

* ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ìˆ˜ì§‘ì€ CDCë¥¼ í†µí•´ ì¶”ì ëœ ë³€ê²½ ì‚¬í•­ ë•ë¶„ì— Steamì—ì„œë§Œ ë¡œê·¸ ë°ì´í„°ë¥¼ ì½ê¸° ë•Œë¬¸ì— ì†ŒìŠ¤ ì‹œìŠ¤í…œì˜Â **ë¶€í•˜ë¥¼ ì¤„ì´ëŠ”**Â ê²ƒê³¼ ê°™ì€ ë” ë§ì€ ì´ì ì´ ìˆìœ¼ë©°, ì´Â **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤**ì— ì „ì ìœ¼ë¡œ ì˜ì¡´í•˜ì—¬ ë‹¤ë¥¸ ë§ì€ ì‹œìŠ¤í…œì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì¬í˜„í•˜ê³  ì¬ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

* ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤ëŠ” Data Engineerì˜ ëª¨ë“  ë°ì´í„° ì†Œë¹„ìì—ê²Œ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì¼ ë¿ì…ë‹ˆë‹¤. ë‚¨ì€ ì‘ì—…ì€ ì‚¬ìš©ìê°€ ë§ˆì§€ë§‰ì— ì „ì²´ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡ CDCì—ì„œ ì–´ë–»ê²Œ ì¬êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì‚¬ì—ì„œëŠ” ì´ CDC ë°ì´í„°ë¥¼ ì½ëŠ” ë° ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì¸Â [ì“°ê¸° ì‹œ ë³µì‚¬](https://hudi.apache.org/docs/next/table_types/#copy-on-write-table)Â ë°Â [ì½ê¸° ì‹œ ë³‘í•©](https://hudi.apache.org/docs/next/table_types/#merge-on-read-table)Â .